"""*********************************************************

NAME:     ex4

AUTHOR:   Paul Haddon Sanders IV, Ph.D

VERSION:  2. Backpropagation

*********************************************************"""

from sklearn.linear_model import LogisticRegression
from pdb import set_trace as pb
from scipy import io
from PIL import Image as im
import matplotlib.pyplot as plt
import numpy as np

### Get data ###############################################

# There are 5000 training examples.
# Each training example is a 20x20 pixel image of the digit.
# Each pixel is represented by a number indicating
# greyscale intensity.
# This 20x20 grid of pixels is unrolled into a
# 400-dimensional vector.
# Then X is a 5000x400 matrix.
# Y represents the labels for each X training matrix.

mat = io.loadmat('ex4data1.mat')
X   = mat['X']
Y   = mat['y']
Y   = Y.reshape(Y.shape[0])

### Reshape Y ##############################################

n_class_labels = np.unique(Y).shape[0]
Y_exp = np.zeros((Y.shape[0],n_class_labels))

for i,val in enumerate(Y):

    # we need to change the Y outputs into an array, since
    # our classifier is with many different options, not
    # only one digit.
    
    index = val - 1
    
    Y_exp[i][index] = 1

Y = np.copy(Y_exp)

### Sizes of matrices ######################################

# X      = [S,M]
# theta1 = [N,M+1]
# a2     = [S,N]
# theta2 = [K,N+1]
# a3     = [S,K]
# Y      = [S,K]

S   = X.shape[0]
M   = X.shape[1]
K   = n_class_labels

# S = total number of training examples
# M = total number of features, excluding the bias unit
# N = total number of units in the hidding layer, excluding
#     the bias unit
# K = total number of output classifications

### 2.1 Sigmoid gradient ###################################

def sigmoid(z):

    return 1. / (1. + np.exp(-np.clip(z,-250,250)))

def sig_grad(z):

    return sigmoid(z) * (1 - sigmoid(z))

sig_grad_output = sig_grad(np.array([100,0]))

### 2.3 Backpropagation ####################################

def activation(X,theta):

    z  = np.dot(X,theta[1:]) + theta[0]

    return 1. / (1. + np.exp(-np.clip(z,-250,250)))

def predict(X,theta1,theta2):
    
    hidden_layer = np.zeros((X.shape[0],theta1.shape[0]))
    output_layer = np.zeros((X.shape[0],theta2.shape[0]))

    for i in range(theta1.shape[0]):

        hidden_layer[:,i] = activation(X,theta1[i])   

    for i in range(theta2.shape[0]):

        output_layer[:,i] = activation(hidden_layer,theta2[i])
    
    return hidden_layer,output_layer

def cost_unregularized(X,Y,theta1,theta2):

    S = X.shape[0]
    
    activate = predict(X,theta1,theta2)[1]

    J = -1 / S * (Y * np.log(activate) + (1-Y) * np.log(1-activate)).sum()

    return J

def cost_regularized(X,Y,theta1,theta2,lambd):

    S = X.shape[0]
    
    activate = predict(X,theta1,theta2)[1]

    J = -1 / S * (Y * np.log(activate) + (1-Y) * np.log(1-activate)).sum() + 0.5 * lambd / S * ((theta1[:,1:]**2).sum() + (theta2[:,1:]**2).sum())

    return J

def dcost_unregularized(X,Y,theta1,theta2,eta):
    
    S = X.shape[0]
    
    a2,a3 = predict(X,theta1,theta2)

    ###

    delta3      = Y - a3

    Delta2_all  = delta3.T.dot(a2)
    Delta2_bias = delta3.T.sum(1)
    Delta2      = np.append([Delta2_bias],Delta2_all.T,axis=0).T

    dtheta2     = eta * (1 / S * Delta2)

    ###

    delta2      = delta3.dot(theta2[:,1:]) * a2 * (1 - a2)

    Delta1_all  = delta2.T.dot(X)
    Delta1_bias = delta2.T.sum(1)
    Delta1      = np.append([Delta1_bias],Delta1_all.T,axis=0).T

    dtheta1     = eta * (1 / S * Delta1)

    return dtheta1,dtheta2

def dcost_regularized(X,Y,theta1,theta2,eta,lambd):
    
    S = X.shape[0]
    
    a2,a3 = predict(X,theta1,theta2)

    ###

    delta3      = Y - a3

    Delta2_all  = delta3.T.dot(a2)
    Delta2_bias = delta3.T.sum(1)
    Delta2      = np.append([Delta2_bias],Delta2_all.T,axis=0).T

    dtheta2     = eta * (1 / S * Delta2 + lambd / S * theta2)

    ###

    delta2      = delta3.dot(theta2[:,1:]) * a2 * (1 - a2)

    Delta1_all  = delta2.T.dot(X)
    Delta1_bias = delta2.T.sum(1)
    Delta1      = np.append([Delta1_bias],Delta1_all.T,axis=0).T

    dtheta1     = eta * (1 / S * Delta1 + lambd / S * theta1)

    return dtheta1,dtheta2

### 2.5 Regularized Neural Networks ########################

def fit(X,Y,eta=3.0,lambd=0.0,n_iter=200):
    """Fit training data.

    Parameters
    ----------
    X : {array-like}, shape = [S,M]
      Training vectors, where S is the number of samples 
      and M is the number of features.
    Y : {array-like}, shape = [S,K]
      Target values, where K is the number of classifications.

    Returns
    -------
    theta1 : {array-like}, shape = [N,M+1]
      Weights for the activation from the input layer to
      the hidden layer.
    theta2 : {array-like}, shape = [K,N+1]
      Weights for the activation from the hidden layer to
      the output layer.
    costs : list
      Value of the cost for each iteration.

    """

    N = hidden_nodes
    M = X.shape[1]
    K = Y.shape[1]
    
    theta1 = np.random.normal(loc=0.0,scale=0.7,size=(N,M+1))
    theta2 = np.random.normal(loc=0.0,scale=0.7,size=(K,N+1))

    costs = []

    pb()
    
    for _ in range(n_iter):
        
        dtheta1,dtheta2 = dcost_regularized(X,Y,theta1,theta2,eta,lambd)
        theta1 += dtheta1
        theta2 += dtheta2

        cost = cost_regularized(X,Y,theta1,theta2,lambd)

        print('cost = {}'.format(round(cost,7)),end="\r")
        
        costs.append(cost)
        
    return theta1,theta2,costs

### 2.6 Learning parameters using fmincg ###################

hidden_nodes = 25

theta1,theta2,cost = fit(X,Y,3.0,0.0,2000)

guess = predict(X,theta1,theta2)[1]

correct = round(sum(np.argmax(guess,axis=1) + 1 == np.argmax(Y,axis=1) + 1) / S * 100,1)

### output #################################################

print("   ex4 : 2. Backpropogation")
print("   Amount Correct = {}".format(correct))


